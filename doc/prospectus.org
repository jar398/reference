#+TITLE: Analyzing compositional semantics
#+latex_header: \hypersetup{colorlinks=true,linkcolor=blue}
#+latex_class_options: [12pt]


* Forward

This document is not intended to be a complete report but rather an
outline or prospectus.  Its purposes are to get high-level feedback on
the project and to help recruit collaborators and readers for future
versions of this document.

To criticize or copy-edit something so unfinished and imperfect would
be very easy.  I'm not sure it's a good use of time given that
whole sections may be dropped or rewritten.  The kind of help I'm
looking for is advice on what is being said and on the overalll
structure of the exposition.  Does it make sense?  Is it nontrivial?

* Introduction

Compositional semantics refers to a situation where a sentence
composed of parts (words, phrases) comes to have a meaning that
derives from the meanings of the parts, as opposed to a meaning that
is bespoke to that sentence.  Use of compositional semantics is the
distinctive and most consequential way in which the human species
differs from other living things.  Because of it, sentences that have
never before been uttered can be understood by agents that have never
heard them. My aim here is to explain how to reduce compositional
semantics to physical phenomena, freeing it from magical, subjective,
or "intentional" explanations, subjecting it to scientific study, and
enabling principled use in engineering.

(It might be helpful to review the SEP article on compositionality:
https://plato.stanford.edu/entries/compositionality/)

/Reference/ is the central roadblock in accounting for
compositional language.  If we knew what reference was, we could
explain the mechanics of compositional language.  The main motivating
question for the theory is the nature of reference, made operational
by the following question:

    #+BEGIN_QUOTE
    Suppose somebody claims that a phrase N refers to some
    object O.  How do we assess the validity of such a claim?  What
    evidence do we bring to bear one way or the other?
    #+END_QUOTE

However, the theory goes beyond reference, and to the extent it
succeeds, it should shed light on a variety of puzzles around
language.

The core of the exposition is in four parts.  Part 1 presents an
'ontology', which can be taken to be either the description of a
contrived world, or an abstraction of the real world.  Part 2 talks
about agents, which are objects that 'act' in and on the world and
interact with one another.  Part 3 is about communication between
agents.  Finally, part 4 gets to the main topic of compositionality.

* World

By 'the world' I mean as much of reality or imagined reality as we
care to consider, here and now, for expository purposes.  Since the
goal is to relate meaning to empirical observation, we might call it a
socio-physical world.

The world can be in various states (or conditions or arrangements).
That is, 'what the world is like' or 'the state of the world' varies.

** Possible world states

I may make a distinction sometimes between 'potential' world states
and those that are 'actual'.  Actual world states are those that the
world actually takes on, while potential world states are those that
can't be ruled out, given what we know (or have assumed) about the
world.

I don't want to make too much of this, since this is not a treatise on
probability theory or model logic.

** Variable

A variable is a function from world states to values (also called
states or variable states) The function is sensitive perhaps not only
to the given particular state, but perhaps also to some or all states
previous or future to it.  (Instead of present, past, and future time
we could consider any other independent variable.  Since the choice of
independent variable doesn't matter I'll take it to be time.)  The
value of a variable could be a truth value (true/false), number,
letter, number, or anything similarly simple and abstract; a 'scalar'
if you will.

Simple not very formal examples:
    * the brightness of a star as seen from near a particular
      other star (if the world has stars)
    * the reflectance of part of some surface, such as a patch of ground
    * the mass of a piece of fruit
    * the number of zebras on earth
    * for a time t, the number of zebras that have lived up until time t
    * the sugar content of the nectar in a flower

A proposition is a variable whose value can only be true or false for
any given time; that is, a variable bit.

** Encoding variables using multiple propositions

A variable that takes on a finite number of values can have its values
encoded as a set of propositions, making propositions and finite-range
variables 'iso-ontic' or interdefinable.  For a variable taking on
eight values, for example, one encoding might be the usual binary
encoding as three propositions (variable bits).  A different encoding
that might be useful in some situations would be to encode it as eight
propositions, one for each value, with the proviso that exactly one
of them is true at any given time.

In reasoning with partial information about the world, it is sometimes
useful to speak of the probability distribution of a variable assuming
some propositions that we think we know or that some other party
thinks they know.  These propositions or 'prior knowledge' would be
one way to distinguish 'possible world states' from impossible ones:
the possible ones are those consistent with prior knowledge.

I use this interdefinability as justification for being able to choose
a variable-centered framework or a proposition-centered one as is
convenient in context.

** Variables and places

There are many ways a world might be structured, but I find it helpful
to think about structuring by geometry because geometry carries such a
rich set of intuitions and formal methods.

Suppose the world has some kind of geometry, meaning there are
multiple places in it.  Furthermore suppose that there is stuff in the
world, meaning that places have one or more associated quantities or
'properties' (mass, density, temperature, color, texture, and so on).
We can take each property of each location to be a variable.  E.g. if
the location is a part of a wall, and someone paints the wall, the
color of the wall has varied, i.e. the value of the variable that is
the wall color has changed.

(I like to use colors in examples because there are several of them,
they are called by short familiar words, and they are easy to
visualize and to draw.  Henry reminds me that color is a very complex
phenomenon resulting from the interplay of illumination, reflectance,
background, context, perceptual machinery, and so on, but that is not
the kind of color I am talking about.  I'm referring to the kind you
find on or in ordinary children's blocks and paints in good lighting
conditions.)

The world state in this view would consist of all properties of all
locations.

Many things vary, not just properties of places over time.  The value
of a variable at time t-1, given a time t, also varies, as do sums,
products, and integrals of variables, etc.  So when I say 'variable' I
mean quite a broad class - anything that varies according to time.
(And perhaps even according to the choice of world, if we start to
think about alternative worlds, but let's not go there.)

** Constraints

As hinted above, in a particular world, there may be world states
(general states) that are not possible.  This may be due to
constraints between variables.  For example, if P and Q are
propositions, there may be a constraint that says that exactly one of
P and Q can be true.  Constraints can hold between collections of
variables.  We previously saw an encoding that depended on a simple
constraint, that exactly one of eight propositions must be true.

A functional constraint is one where knowing the values of 'enough'
variables tells you the value of some others.  E.g. is a, b, and c are
constrained to have a sum of zero, then knowing any two tells you the
third, i.e. the third is a function of the first two.

The idea of constraint it related to those of dependence and
correlation.  The statistical notion of mutual information
[[https://en.wikipedia.org/wiki/Mutual_information]] also describes a
situation where knowing information about some variables gives you information
about others (or at least about their distribution, in a
probabilistic setting).

A variable can be involved in multiple constraints, resulting in a
constraint network.

Another way of speaking about constraints is to say that the value of
one variable is constrained by the values of others.  In a set of N
mutually constrained variables, each of them is constrained by the
set consisting of N-1 remaining variables.

When I speak of constraints I am speaking of properties of the world
that might be confirmed or refuted by data spanning changes to variables.
Constraints might or might not be purposefully created.

** Clusters

Constraints combine when they share variables.  We call the resulting
systems constraint networks.  While everything affects everything else
to some degree, we can usually identify more or less discrete networks
consisting of constrainted parts, which can also be described as
clusters of mutually constrained variables.

These clusters tend to be associated with "things" or "objects" or
"entities".  Their variables reflect the condition of their parts or
aspects, and their constraints reflect the arrangement and function of
the parts.

I'll use "object" as it is least awkward, even though "entity" or
something else might be better.  "Object" implies more of a commitment
than I would like; I do not mean to imply by it a connected physical
obect with clear boundaries, or anything quite so crisp.  An object
could be any collection of constrained variables: a cloud, a city, a
shadow, etc.

The point of all this analysis is to say that variables (including
propositions) are the correct starting point for ontology, and
objects are secondary since they emerge from ensembles of variables
and their constraints.  You can take this as a claim about our world,
or about the contrived world I am presenting, or as a philosophical
position.

** Symmetry

The geometry of a space is characterized by its symmetries, which in
this case would be transformations that take a set of locations (and
their properties) to a new set of locations, preserving important
aspects of their geometry.  For example, symmetries that conserve
distances, angles, and (in a three dimensional space) chirality, would
be combinations of rotations and translations.  Distances and angles
are constraints, so it is natural to ask of any constraint whether it
is preserved by a transformation.  By saying that "the same
constraint" applies across a transformation, we are saying that a
constraint might apply not just to a particular ensemble of variables,
but rather to an ensemble of variables that is itself variable -
different variables at different times or in different places.  In
particular, a geometric symmetry takes us from one set of locations to
a new set of locations.

One can define variables that 'track' symmetries.  Suppose we have a
symmetry between a variable $x$ before time $t$ (or before some kind
of event) and a variable $y$ after $t$.  We can define a new variable
$z$ that coincides with $x$ before $t$ and with $y$ after $t$.  $z$
"tracks" $x$ to $y$ across the symmetry.

This extends to tracking of constraints.  The correct choice of
symmetry will 'preserve' constraints.  We might say that the
constrained variables appear to have 'moved'.

We may choose to analyze the world, or part of it, in a new coordinate
system based on a transformation that tracks variables and constraints
that we care about.

* Agents

An agent is an information processing system of some sort: something
that acts in response to, and on, its environment. The mechanism is
not passive like a rock or hammer, but active in that the environment
is sensed and actions are chosen in a way that is sensitive in a
complex way to what is sensed.

Examples: robot, human, vervet monkey, character in a video game.

** Sensors and actuators

'Act on the environment' means exerting a force, either substantial force
such as locomotion or breaking something, or gentle force such as
generating a sound,
writing on a piece of paper, 
altering the voltage level on a wire, 
or emitting light from a display,

A particular action that an agent can take is accomplished with some
part of the agent (organ, motor, muscle, light etc); such an agent
part is called an 'actuator'.

To be able to act differentially based on what its environment is
like, an agent also has to be able to detect forces that are applied
to the agent.  

An agent therefore transduces information from its environment,
together with its memory of what has happened before, to form
additional memories and/or to transmit information to its environment.

Thus, agents have parts we'll call 'sensors': detectors of light,
sound, touch, etc.  A sensor has the function of establishing a
constraint between a variable outside the agent and one inside the
agent (in the sensor).

Actuators are the same but in the opposite causal and temporal
direction.  An actuator estabishes a constraint between a variable
inside the agent (what it "wants to do") and one outside the agent
(some change that is effected).

** Virtual sensors and actuators

An agent may take in sensor information in a series of processing
steps (and, dually, activate an actuator following a series of
processing steps).

At the agent/environment interface, there is a physical linkage
between the state of some part of the environment and the state of
some part of the agent.  The environment-adjacent agent part
is a sensor.

For any sensor, and any state the sensor might take on, it is useful
to consider the variable whose value at any time is the sensor's state
at that time.

Typically there is 'circuitry' to process and combine signals coming
from sensors.  The output point of such circuitry is a manifest
variable which, because its value/state is derived from sensor states,
might be called a 'virtual sensor'.  For simplicity I will simply use
the word 'sensor' for either a sensor or a virtual sensor.  If readers
object I will reconsider this terminology.

['virtual sensor' is maybe not a good term.  think about this.]

[not to head off complaints about attenuation, feedback, and so on.
not relevant.]

The same reasoning works in reverse to yield the idea of a virtual
actuator, whose action devolves into the action of more actual
actuators.

An example of such circuitry is tracking.  As something in the
environment moves, or as the agent or one of its sensor-carrying parts
(e.g. eye or ear) moves, the agent may have virtual sensors whose
values correspond to the position or other properties of the moving
object.  The virtual sensor is a complicated function of actual
sensors.

Some important kinds of sensation may be elicited by the agent
performing an 'experiment', meaning that an actuation/sensation
sequence results in a virtual sensor yielding information not
available in other ways.

** Payoff

Agents may derive benefit or harm from what happens to them, including
their own actions.  The benefit or harm is detected through their
senses, perhaps with some delay.  I think of the payoff as a numerical
quantity, intended to model fitness (in an evolved species), profit
(in a commercial product), points (in a game), happiness, etc.  But I
do not care to develop this formally.

An agent will, other things being equal, tend to choose the highest
payoff (or expected payoff) action, if it has a choice.

** Cooperation

When two agents interact, the interaction is cooperative if and only
if the expected payoff (under justifiable expectations) to both agents
is positive.  I take this as an axiomatic definition of cooperation.

When the payoff is positive for one agent but not the other, the
interaction is exploitative.  We tend to call this "cheating".  
Voluntary non-cooperative interactions tend to be extinguished over
time, since the exploited agent will tend to learn to stay out of them.
Such an interaction pattern cannot be maintained indefinitely without
coercion.

One might be tempted to judge cooperation by intent; that is, an agent
might intend to produce positive payoffs, but might 'make a mistake'
or 'be the victim of bad information' or the interaction might not
turn out well due to 'bad luck'.  This is really a kind of
amortization.  We might still call their behavior cooperative due to
/expected/ payoffs over time.  If cooperation were the focus of this
prospectus, it would be important to distinguish short-tem payoff from
expected payoff.

** Object hypotheses

Common sense suggests that agents perceive objects, but objects are
not sensed or manipulated directly.  The agent's sense that there is
an object has to be explained in terms of the apparatus it has on hand
(sensors and actuators), just as the objects themselves had to be
explained in terms of variables and constraints.

Sensors receive information from the agent's environment by relaying
state across the agent/environment boundary.  The agent can detect
which variables (thus read) 'go with' which other ones (are predictive
of the others, similar to 'correlated'), and we can suppose that they
form 'object hypotheses' consisting of variables that they know about
that seem to go together (are mutually constrained).

Object hypotheses help agents make predictions, and better predictions
lead to higher payoffs (following).  Given enough variable
observations to 'identify' a particular constraint network, other
unobserved variable values may be predicted.

Prediction is the entire purpose of object perception, the reason that
agents have such a facility at all.

* Communication
** Channel

A channel connects two agents A and B so that they can interact.  One
agent, the 'speaker' or 'sender' or 'writer', can change the state of
the channel, and the other, the 'listener' or 'receiver' or 'reader',
can sense the state.

B is thereby connected indirectly to A's actuators, and A is connected
indirectly to B's sensors.  The forces involved are typically gentle.
Communication, being gentle, does not result in any direct physical
payoff or loss to the participants [notwithstanding the 'handicap
principle' and expensive media; TBD].

** Sentence

The state of a channel is called a 'call' (as in, bird or monkey call)
or a 'sentence' or a 'message'.

An 'atomic' sentence is one without independently meaningful parts
(e.g. the call of a vervet monkey, cry of a baby, or an emergency word
like "help!").  A 'compound' or 'composed' sentence is one with parts
(as in a multi-word sentence uttered by an adult human or robot).

** Sayability

Suppose A is communicating with B over a channel.  A sentence is
sayable in a context if, when A says it, the outcome is a cooperative
(positive payoff) interaction between A and B.

It is useful to call out special configurations of sayability:

A positive payoff to B can result if the sentence 'provides useful
information'.  A might act, in effect, as an extension of B's sensors,
lending its sensors to B.  Such sentences are called /declarative/;
they primarily have an expected positive payoff to B, who is in a
position to make use of the information.  (A may receive an indirect
positive payoff via reciprocation, inclusive fitness, amortization, or
in some other way.)

A positive payoff to A can result from B doing something on A's
behalf.  B might act, in effect, as a new actuator for A.  We call
these /imperative/ sentences.  They have an expected positive payoff to
A as a result of the action, and an indirect payoff to B.

A /question/ is a kind of imperative sentence, one that requests
information (an answer).

Again, as in any cooperative enterprise, sayability is to be
determined based on amortized or average payoffs; it may refer to an
individual interaction, but in general payoff is expected on average.

Conventionally we would speak of a sentence being true, rather than
being sayable, but there is no effective, empirical way to assess
truth other than by looking at whether the sentence has a 'good'
vs. 'bad' payoffs.  Sayability is an idea that makes sense in terms of
biology and evolution; it does not require appeal to cognition or
metaphysics.  This is not to say truth is meaningless or arbitrary; it
is just not helpful in this analysis to attribute it to the agents'
communication.

In many situations it would be natural to use sayability as evidence
of truth, and non-sayability as evidence of falsity, so it is easy and
probably not too harmful to confuse sayability and truth.

Sayability may not be directly observable, but we can gather evidence
about it.
  1. If an agent says S, it is probably sayable (in that context).
  2. If an agent does not say S when otherwise it might, maybe it's
     not sayable.
  3. If we have a way to ask an agent whether it thinks it would be OK
     for it to say S (i.e. whether S is sayable), we might simply ask it.
  4. Of course, we can try to measure payoffs directly.

Whether sayability is a property of a sentence depends on whether the
region in question contains variation in the meaning of the sentence
(e.g. if there are multiple languages, or if meaning varies depending
on which speakers/listeners are involved).

** Sentence meaning

The meaning of a sentence is a proposition; specifically, a
proposition that is true if and only if the sentence is sayable.

Presumably the sentence is sayable (or not) /because/ the proposition
is true (false), but such causation would usually be complex.
Fortunately we don't need to understand what the causation is.

** Example: vervet monkey (signaling systems)
* Composition
** Sentence parts

Sentences in natural language come in a variety of compositional
forms, but the canonical structure of a subject phrase composed with a
predicate phrase is at the core of language; everything else
(prepositional clauses, conjunctions, appositives, etc.) is an
elaboration.  I will stick to the canonical form because my aim is
only to explain reference, not all of language.

** Reference

We come to the motivating question now: Suppose somebody makes a claim
that some phrase N refers to some object O.  How do we assess the
validity of such a claim?

To drive home that this is a rigorous question free of metaphysics, we
can put it in software engineering terms: Suppose a piece of software
is said to use phrase N to refer to some object O.  How do we write a
unit test for that property?  Or, how would we detect a bug in the
program caused by an error in reference?

The theory leads to the following definition of reference:

    #+BEGIN_QUOTE
    A noun phrase N refers to object O iff for every sentence S having
    N as its subject phrase, S means a proposition that is about O.
    #+END_QUOTE

(See above for 'means', 'proposition', and 'about'.)

This would predict, for example, that in learning 'what N refers to', an
agent learns the sayability of a number of sentences S that lexically
include N, and interpolates an object hypothesis (the referent of N)
that goes with the propositions that are the meanings of the sentences
S.

Every part of the theory rests on a foundation of variables, sentences,
and sayability.  These are all external phenomena that can be observed
and measured.  There is no appeal to 'mental models' or 'concepts'.

We are led to this reduction because the theory provides no
other way to define reference.

How well this matches the way "reference" is used in ordinary language
remains to be seen.

** Predication

For compositionality, we need for both subjects and predicates to have
meaning that enables their use in new sentences.  Object hypotheses
liberate noun phrases from the sentences they inhabit and permit them
to join with new predicates, but we must also have some theory of the
independence of predicates.

I've been so busy with reference I haven't had time to nail this down.
But my feeling is that it will end up being much easier than
reference.  My working hypothesis is that a predicate is best modeled
as a 'procedure' that acts quasi-computationally on an object
hypothesis to yield a truth value.

(Actually an agent will have multiple 'competencies' around subjects
and predicates, not just for assessing truth/sayability but also for bringing it
about, as for the interpretation of imperatives.)

An important case to consider is requests to make things.  "Make me an
omelet" has a reference to an omelet that does not yet exist, but will
exist after the request is carried out.  Computationally, the
predicate "Make me ---" operates not on the omelet, but on the omelet
hypothesis.  The hypothesis in turn can be consulted to determine what
ingredients should be used, by asking it what one would observe should
the request be successfully carried out.

** Shared object hypotheses

If two agents are together in a region, they are 'likely' to form
similar object hypotheses when 'looking at' the same parts of the
region, even if they have different types of sensors.  This is because
forces arising from some single 'real' object (one that you and I
would recognize as such) lead to peripheral sensations for both
agents.  Manifest variables in an object hypothesis 'go with'
theoretical variables derived from an object's state.

However, agreement on object hypotheses is by no means guaranteed.
Agents are always dealing with incomplete information and can in good
faith reach different conclusions in the same situation.


* What do we do now
** Assessing meaning and reference

Assays of sentence meaning (sayability) cannot be exhaustive because
we would have to measure payoffs in all possible situations, while
controlling for agents' memories (experience).  This might be possible
in a laboratory setting, but is not practical in any realistic
setting.  We can, however, make pretty good hypotheses of meaning with
limited data, by reasoning about agents and environments (using our own knowledge of
them) and applying common sense assumptions to seek the best hypotheses
of meaning that fit available data.

Similarly, because there are so many predicate phrases that might
combine with a given noun phrase to form sentences, we cannot
enumerate and test them all, and we may have to use heuristics to
determine reference.

These definitions of meaning and reference may be exact, but in
practice, meaning and reference are unknowable.  This may feel
unsatisfactory, but remember that there is no definite knowledge in
science at all, only hypotheses that fit the available data better or
worse than one another.

** Cross-agent sameness judgments
Write me.

Suppose agents A and B both use a phrase $\Phi$.  How do they know, or
determine, whether they mean the same thing by $\Phi$ ?

And especially, if they are trying to make these determinations by
putting $\Phi$ into sentences, how do they know that the phrase
$\Pi$ denoting the predicate means the same to both?

Sounds like a system of simultaneous equations to me.

** Unit tests

[Placeholder.  If I'm right then I've established that a computer
really is capable of genuine meaning and reference, not just "form
filling", but only under certain circumstances - if it understands the
object's constraint network.  I should be able to spell out the
implications of the theory for 'knowledge representation' and robot
language.]

** What does this have to do with HTTPrange-14?

The infamous HTTPrange-14 question (World Wide Web consortium,
2003-2011) hinged on what a particular kind of URL (or URI) refers to,
and years of bickering by many very clever people didn't lead to any
progress on the question.

https://en.wikipedia.org/wiki/HTTPRange-14

E.g. does the URL https://en.wikipedia.org/wiki/Magna_Carta refer (in
a language in which URLs refer) to the Magna Carta, the document
prepared circa June 1215, or does it refer to a Wikipedia article
about the Magna Carta?  The things you say about the two objects are
incompatible, so to avoid confusion it would be helpful to know which
was intended, and to assist machine processing it would be helpful if
the intent were agreed on in a systematic way, not just for this one
URL.

Well, if it did refer to one of the two, how would one know?  Or if we
wanted it to refer to one and not the other, what guidance would we
give to application developers?

Standards are most successful when they are accompanied by good unit
tests, so in order to steer the group away from metaphysics and
bullying, I asked the question, how would someone write a unit test to
detect variance against /any/ requirement having to do with reference?
I didn't receive any answer.

* Prior work

Leibniz, Frege, Russell, Wittgenstein, Skinner, Quine, Chomsky,
Kripke, Horwich, Millikan, Brian Cantwell Smith, Gopnik, Yablo, Martha
Gibson (/From Naming to Saying/), Bogdan (/Predicative Minds/),
Boersema (/Pragmatism and Reference/), Reddy ("conduit metaphor"),
many others.  Maybe Austen.

Criticizing the existing literature is like 'shooting fish in a
barrel'; none of it is very good in my opinion.  This is why I'm
putting off the task of doing a literature review.

* Acknowledgments

I'm much indebted to Brian Cantwell Smith, Henry S. Thompson, Alan
Renear, and Gerry Sussman for ongoing encouragement.

Pat Hayes advised a working group I participated in on matters of
meaning and reference and in doing so taught me a lot about model
theory.  He was a major inspiration for this work.

Thanks to Christine Lemmer-Webber and Alan Ruttenberg for comments.


* Other topics TBD

This section is a junkyard of object-related topics.

There's a lot of fun material here but it is probably not all
necessary to the narrative.

Need to figure out a natural ordering of these topics, or find places
in the previous text where they might fit, if I do decide to include
them at all.

I might flush all of this.

** Dualities

Agent / environnment

Sensor / actuator

Input / output

etc.

** Continuity and tracking

Why do I want to introduce continuity? -- continuity is
deeply tied to clusters, and should come in later.

The cups and ball game, or shell game, or some 'simpler' variant gives
a good example of the importance of continuity.

** Goes with - objects - needs work

FLUSH / REWORK without mentioning objects

Unlike in many conventional treatments (e.g. Aristotle, Leibniz, BFO),
objects are absent from this model of the world so far.
This is because of the intent to understand what an agent is up
against in living and communicating in the world, and agents do not
have a direct way to detect objects.  Objects cannot be sensed or
measured; they have to be imputed from experience.

In the conventional view, we say there is an object x, and it has
properties P1(x), P2(x), and so on, and therefore it has property
P(x).  Here we cannot start with an object.  Rather we have:

  1. first, a set of observations Q1, Q2, and so on,
  2. then we connect them to one another; the pattern of connection we
     identify as an 'object' x,
  3. only then can we say that Q1 = P1(x) i.e. the proposition Q1 says
     that x has property P1, or is 'about' x.

If the mutual information is high, i.e. if knowing X is nearly as good
as knowing Y, I'll say that X "goes with" Y or that X and Y "go
together".

** Proxies

If it is difficult or dangerous to read a variable, say x, directly,
it can be helpful to identify a second variable y that can provide information
about x.

For example, it might take time and energy to determine the sugar
content of nectar hidden deep inside a flower, but other variables,
such as the flower's color or shape, can act as a less 'expensive'
proxy for the desired information.

We can come to know constraints such as these through 'experience',
i.e. by watching how they change through time.  If they change
together we have a constraint, and if they change independently they
don't.

If we were to observe these three variables - nectar sugar content,
color, and shape - were unconstrained, we might just say that it is
because they are all part [trails off]

** Haecceity / unknowns
We like to track things because the thing carries something of value:
either information or assets (e.g. food).  Or, we might want to track
speculatively, based on expectation that tracking will pay off.
** Recovering objects from propositions

[I need to rewrite this for approachability.  I need an example where
we have a set of variables that are sensors or detectors aimed at some
part of the world, and various objects come into that part of the world
and are sensed, and by 'identifying' one we come to know properties
that are otherwise expensive to detect.

And maybe the objects described here - constrained variable sets -
should not be called 'objects' even if they are isoontic with
objects.  But then what they should be called?]

From a set of variables K = {X1, X2, ... Xn} we can ask, what other
variables can be predicted from the variables in K (over some region)?
The collection of all such variables would be a larger set K'
containing K.

Since variables that are merely functions of the variables in K cannot
have any new information, we are really asking about regularities in
the system: what can we predict about the system based on what we
already know?

I hypothesize that what you and I would identify as an object in
the system corresponds to a set K' of variables with mutual
information; and vice versa, if we have a maximal set K', then there
is a corresponding object.  That is, maximal mutual dependent variable
sets are equivalent to objects.

[Yes I'm playing fast and loose with the cardinality of K'.  There is
also some question about the properties of these maximal collections
K': are they unique etc.  Future work.]

If K' doesn't add 'much' to K, i.e. if K doesn't predict much, then we
don't have much of an object.  It would be nice to have a way to
exclude random collections of variables as identifying sets.

Note that multiple distinct sets K, even minimal ones (no subset
also identifies K'), could identify the same object K'.  That is, an
object might be identified in a variety of ways.

When we assess mutual information there are several kinds of
variation over which we might sample:
  1. All or most of the variables change at once; this could be due to
     the variables coming under control of some common object (it
     'moves into view').
  2. When an object is replaced by a similar but
     distinguishable cause.
  3. A change to an object that doesn't
     reflect replacement with a different object, such as when a cuttlefish changes
     its color, or a computer monitor displays a different picture.
  4. Noise, as when a sensor is itself probabilistic.

[TBD: mutual information when we're dealing with multiple variables is
mathematics I don't understand - this part needs to be tightened up]

One motivation for a probabilistic approach with multiple regions is
that it allows objects to change without immediately becoming
unrecognizable.  (The Ship of Theseus paradox has different answers
depending on the choice of region.)
>>>>>>> de498eb531d612727bf2b0759ad6d44d701a5a8c

** Identity and Leibniz

[digression into Leibniz - if you know something's properties, you know
whether the something is some given object - or, an object is determined
by its properties]

** Choosing (orienting to) a subject...
** Decomposing proposition into predicate + subject
Write me.  I define 'predicate' and 'subject' here are 
semantic, not syntactic.  (The syntactic terms would ne
'predicate phrase' and 'subject phrase'.)
*** Property

A variable is a property of an object if has a dependence on the
object's other properties.  (or something like that.)
????
A variable is a property of an object if it belongs to the object's
variable set K'.  (?)

** Aboutness

A variable (and in particular a proposition) is about an object iff
its value is sensitive to the properties of the object, i.e. there is
some change to one of the object's properties that could
cause the value of the variable to change.

Aboutness is similar to propertyhood in being a relationship between
propositions and objects, and the latter implies the former.

[As usual, choice of region is important.]

** Gestalt and mereology
Maybe this goes in some other section.

An object, and a part of that object, are different entities.  Perhaps
the part is a subnetwork of the whole's constraint network.
Explain.

** Coordinate systems and effective variables

Suppose that the world has some kind of space with a geometry.
Consider a variable defined by a coordinate system on the space
together with operations that can be performed in reference to the
coordinate system.

** Objects change

In order to make use of an object hypothesis when appropriate an agent
must be able to discriminate situations where the hypothesis is likely
to work (the object is 'identified') and those where it is not (what
is seen is not 'identified' as the object).

The theory implies some position on the Ship of Theseus.  What is it?


** Drifting
write me

consider a world without a fixed substrate, in which the agent drifts,
and other things drift by it.  Locations are not unconstrained, but
are relativey so.  What does this do to the pressure and/or ability to
make use of compositional semantics?  Would we get
compositionality in an ocean ecosystem?...

** Species (generic individuals)

Canonically we think of reference as being to particular physical
individuals, but in ordinary language it is also common to refer to
generic individuals such as The Redwing Blackbird - not referring to
the class, but to a single idealized, canonical, or archetypal bird.
When I was in Peru I was told that all capybaras were called Charlie
and all toucans were Poncho, almost as if there was only one of them.
(but perhaps these references were more like pronouns than generic
individuals, I don't know.)  Is either of these two modes prior?

** Cheating

Suppose A says something and B acts in response.  If A receives a
positive payoff but B receives a negative payoff (i.e. penalty), we
might say A 'lied' to B or A 'tricked' B or A said something that
wasn't 'true'.  (It's also possible that A made a mistake.)

If A receives a negative payoff and B a positive payoff, we could say
B 'betrayed' A by performing an action not favorable to A, when A
trusted B to perform the favorable action it expected.
(But it's also possible that B made a mistake).

** Community

In principle, language could be negotiated independently between each
pair of communicators, but in practice the is little cost and enormous
benefit for an agent to be able to use the same language with multiple
speakers.  Doing so reduces learning time and the potential for
mistakes.  An agent can learn language from one source and then
practice it with another.  In a community of language users there may
be discrepancies to deal with between different communicator pairs,
but these can be treated either as inconsistencies to be 'corrected',
or as exceptions that just have to be remembered.

** Language

A language is a set of practices used by individual agents in
communicating with another agent.  From the perspective of this point
in the exposition, a language would be simply a correspondence between
a number of sentences and their meanings, but we would want to expand
this to other practices as we look further.

** Semantics and power struggles

An interaction can 'go bad' in that agent A can say something,
expecting a positive payoff to both A and the listening agent B, but
one or the other payoff turns out to be zero or negative.  The 
payoffs depend on the behavior of both agents ('saying the right thing' and
'doing the right thing'), so it is possible that the payoffs can be positive if
either A or B changes its behavior.  There may be a choice to be made
between A changing or B changing, if either change will lead to
positive payoffs.  In this situation there can be a negotiation to
determine which one changes.

In some cases negotiation is impossible because feedback
is impossible or rejected, but suppose that it is possible.

Typically neither agent really wants to change.  The consequences of
the negotiation go beyond just this one interaction since the changing
agent will have to decide whether to apply its change to future
interactions with the other agent, and to its interactions with other
agents.

If the negotiation leads to A changing what it says, it might be
described in normative terms as "A said the wrong thing to B, it
should have said this other thing".  If it leads to B changing what it
does, it might be described as "B misunderstood what A said, it should
have understood it in this other way".

A power imbalance between A and B might determine the outcome of the
negotiation.  If A has more power than B, then it may feel it does not
need to change what it says, and will pressure B to "capitulate" by
changing its behavior.  If B has more power, it may feel it does not
need to change what it does, and it will pressure A to "capitulate" by
changing what it says.

** Child development

Infants learn meaning quickly and apparently with very little data.
Is what an infant does consistent with what I've outlined?

