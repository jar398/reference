Demystifying compositional semantics - a prospectus

* Forward

This document is not intended to be a complete report but rather an
outline or prospectus.  Its purpose is to help recruit collaborators
and readers for future documents.

* Introduction

Compositional semantics is a system whereby a sentence composed 
of parts (words, phrases) comes to have a meaning that derives from
the meanings 
of the parts.  Use of compositional semantics is the distinctive and
most consequential way in which the human species differs from other
living things.  Because of it, sentences that have
never before been uttered can be understood by agents that have never
heard them. My aim here is to explain how to reduce compositional
semantics to physical phenomena, freeing it from
magical, subjective, or "intentional" explanations,
subjecting it to scientific study, and enabling principled use in
engineering.

[[see SEP article on 
Compositionality][https://plato.stanford.edu/entries/compositionality/]]

This document describes a theory that is intended to make particular
predictions about how compositional semantics works.  It should be seen as making
approximate, not accurate, claims about the real world.

Reference turns out to be the central roadblock in accounting for
compositional language.  The crucial question in the analysis is this one:

    #+BEGIN_QUOTE
    Suppose somebody makes a claim that some phrase N refers to some
    object O.  How do we assess the validity of such a claim?
    #+END_QUOTE

The exposition is designed to build up a possible answer to this
question.

* System

By 'the system' I mean as much of reality as we care to consider for
expository and analytic purposes.

At any given time the system is in some condition or arrangement or
state.  'What the system is like' changes over time.

(Yes of course the real world has relativity and so on.  This is not a
theory of everything.)

** Variable

A variable is something that varies.  It takes on various values
depending (on something else).  I will use the word 'variable'
specifically to mean something that varies with time (its value is a
function of time) and whose value is determined by the state of the
system and/or present or future system states (i.e. a pair <all system
states over time, time>).

A "manifest" variable is one whose values are 1-1 with the states of
some part of the system.  Non-manifest variables would occur in our
analysis of the system but not in the system itself.

It is sometimes useful to speak of the probability distribution of a
variable.  We can do that by specifying some region of space-time and
some theory of 'possible region states' representing what could
potentially be the case given some incomplete knowledge state, and
then compute distributions in the usual way.

** Proposition

A proposition is a variable whose value is a truth value (true or false).

The state of the system at time T is 'isoontic' with the aggregate of
all values of all propositions at time T.  That is, states can be
distinguished by propositions.

(Maybe we don't need propositions ?? let's see.)

* Objects
** Goes with

Given variables X and Y and some region, consider the
mutual information of X and Y, i.e. the amount of information about Y
that we obtain by observing X (or vice versa).
https://en.wikipedia.org/wiki/Mutual_information

If the mutual information is high, i.e. if knowing X is nearly as good
as knowing Y, I'll say that X "goes with" Y or that X and Y "go
together".

Just how high the mutual information needs to be in the following is
unclear to me and is probably context dependent.  I will figure this
out later.  For now just assume some threshold I guess.

** Object

From a set of variables K = {X1, X2, ... Xn} we can ask, what other
variables can be predicted from the variables in K (over some region)?
The collection of all such variables would be a larger set K'
containing K.

Since variables that are merely functions of the variables in K cannot
have any new information, we are really asking about regularities in
the system: what can we predict about the system based on what we
already know?

I'm going to call K' an 'object', and K an 'identifier' for the object.

(Yes I'm playing fast and loose with the cardinality of K'.  There is
also some question about the properties of these maximal collections
K': are they unique etc.  Future work.)

If K' doesn't add 'much' to K, then we don't have much of an object.
It would be nice to have a way to exclude random collections of
variables as identifying sets.

Note that multiple distinct sets K, even minimal ones (no subset
also identifies K'), could identify the same object K'.  That is, an
object might be identified in a variety of ways.

** Property

A variable is a property of an object if it belongs to the object's
variable set K'.

** Is about

I want to say: a variable is about an object iff its value is
sensitive to the properties of the object, i.e. there is some change
to the truth of a set of properties that would cause the truth value
of the variable to change.

But this needs to be made more precise.  E.g. what region(s) are we
talking about?

* Agents
** Sensors and actuators

An agent is something that acts on its environment; not passively like a
rock or hammer, but actively.  Examples: robot, human, vervet monkey,
character in a game.

'Act on the environment' means exerting a force, either substantial force
such as locomotion or breaking something, or gentle force such as
altering the voltage level on a wire, emitting light from a display,
or generating a sound.

A particular way in which an agent is able to act is called an
'actuator'.  Think of these as muscles, motors, or lights.

To do anything sensible an agent also has to be able to sense its
environment and detect forces that are applied to the agent.  That is, an
agent transduces information from its environment, together
with its memory of what has happened before, to form additional
memories and/or to decide how to act on its environment.

Thus, sensors: detectors of light, sound, touch, etc.

** Virtual sensors and actuators

An agent may take in sensor information in a series of processing steps.

At the agent/environment interface, there is a physical linkage
between the state of some part of the environment and the state of
some part of the agent.  The environment-adjacent agent part
is a sensor.

For any sensor, and any state the sensor might take on, it is useful
to consider the variable whose value at any time is the sensor's state
at that time.

Typically there is 'circuitry' to process and combine signals coming
from sensors.  The output point of such circuitry is a manifest
variable which, because its value/state is derived from sensor states,
might be called a 'virtual sensor'.  For simplicity I will sometimes
simply use the word 'sensor' for either a sensor or a virtual sensor.
If readers object I will reconsider this terminology.

The same reasoning works in reverse to yield the idea of a virtual
actuator, whose action devolves into the action of more actual
actuators.

An example of such circuitry is tracking.  As something in the
environment moves, or as the agent or one of its sensor-carrying parts
(e.g. eye or ear) moves, the agent may have virtual sensors whose
values correspond to the position or other properties of the moving
entity.  The virtual sensor is a complicated function of actual
sensors.

Some important kinds of sensation may be elicited by the agent
performing an 'experiment', meaning that an actuation/sensation
sequence results in a virtual sensor yielding information not
available in other ways.

** Payoff

Agents may derive benefit or harm from what happens to them, including
their own actions.  The benefit or harm is detected through their
senses, perhaps with some delay.  I think of the payoff as a numerical
quantity, intended to model fitness (in an evolved species), money (in
a commercial product), points (in a game), happiness, etc.  But I do
not care to develop this formally.

An agent will, other things being equal, tend to choose the highest
payoff (or expected payoff) action, if it has a choice.

** Cooperation

When two agents interact, the interaction is called cooperative if the
payoff to both agents is positive.  Otherwise, it is ... not.

When the payoff is positive for one but not the other, the
interaction is exploitative.  Such an interaction pattern can only be
maintained by restricting the "victim's" choices so that the desired
outcome has the highest payoff for them even though that payoff is
negative.

Voluntary non-cooperative interactions tend to be extinguished over
time, since the losing agent will tend to learn to stay out of them.

Ordinarily we would judge cooperation by intent; that is, an agent
might intend to produce positive payoffs, but might 'make a mistake'
or 'be the victim of bad information' or the interaction might not
turn out well due to 'bad luck'.  We might still call their behavior
cooperative.  If cooperation were the focus of this prospectus, it
would be important to distinguish factual payoff from expected payoff.

* Perception

Common sense tells us that agents perceive objects, but this has to be
explained in terms of the apparatus built up so far (variables,
sensors, ...).

Sensors obtain information from the agent's environment by relaying
state across the agent/environment boundary.  The agent
can detect which variables (thus read) go which other ones, and we can
suppose that they form 'object hypotheses' consisting of variables
that they know about that go together.  Object hypotheses help them
make predictions, and good predictions lead to high payoffs.

If two agents are together in a region, they are 'likely' to form
similar object hypotheses when 'looking at' the same parts of the
region, even if they have different types of sensors.  This is because
there are physically dictated peripheral sensations for both agents,
arising from some 'real' object.

And these object hypotheses are similarly likely to be compatible with
actual objects.  Manifest variables in a hypothesis 'go with'
theoretical variables derived from an object's state.

* Communication
** Channel

A channel connects two agents A and B so that they can interact.  One
agent, the 'speaker' or 'sender' or 'writer', can change the state of
the channel, and the other, the 'listener' or 'receiver' or 'reader',
can sense the state.

B is thereby connected indirectly to A's actuators, and A is connected
indirectly to B's sensors.  The forces involved are typically gentle.
Communication does not result in any direct physical payoff or loss to
the participants [notwithstanding the 'handicap principle' and
expensive media; TBD].

** Sentence

The state of a channel is called a 'call' or a 'sentence' or a 'message'.

An 'atomic' sentence is one without independently meaningful parts
(e.g. the call of a vervet monkey or cry of a baby).  A 'compound' or
'composed' sentence has parts (as in a multi-word sentence uttered by
an adult human or robot).

** Sayability

Suppose A is communicating with B over a channel.
A sentence is sayable in a context if, when A says it, the
outcome is a cooperative (positive payoff) interaction between A and B.

A positive payoff to B can result if the sentence 'provides useful
information'.  A is acting, in effect, as an extension of B's sensors.
Such sentences are called declarative.  They have an expected positive
payoff to B.  A may receive an indirect positive payoff via
reciprocation, inclusive fitness, amortization, or in some other way.

A positive payoff to A can result from B doing something on A's
behalf.  B is acting, in effect, as an extension of A's actuators.  We
call these imperative sentences.  They have an expected positive
payoff to A, and an indirect payoff to B.

A question is an imperative sentence that requests information (an
answer).

Conventionally we would speak of a sentence being true, rather than
being sayable, but there is no effective way to assess truth other
than by looking whether the sentence has a 'good' vs. 'bad' effect on
other agents.  Sayability is an idea that makes sense in terms of
biology and evolution; it does not require appeal to cognition or
metaphysics.  This is not to say truth is meaningless; it is just not
helpful in this analysis to attribute it to the agents' communication.

It many situations it would be natural to use sayability as evidence
of truth, and non-sayability as evidence of falsity, so it is easy and
probably not too harmful to confuse sayability and truth.

Sayability may not be directly observable, but we can get close.
  1. If an agent says S, it is probably sayable (in that context).
  2. If an agent does not say S when otherwise it might, maybe it's
     not sayable.
  3. If we have a way to ask an agent whether it thinks it would be OK
     to say S (if S is sayable), we might simply ask it.

** Sentence meaning

The meaning of a sentence is a proposition; specifically, a
proposition that is true if and only if the sentence is sayable.

Presumably the sentence is sayable (or not) _because_ the proposition
is true (false), but such causation would usually be complex.
Fortunately we don't need to understand what the causation is.

* Compositional communication
** Sentence parts

Sentences in natural language come in a variety of compositional
forms, but the canonical structure of a subject phrase composed with a
predicate phrase is at the core of language; everything else
(prepositional clauses, conjunctions, appositives, etc.) is an
elaboration.  I will stick to the canonical form because my aim is
only to explain reference, not all of language.

** Reference

We come to the motivating question now: Suppose somebody makes a claim
that some phrase N refers to some object O.  How do we assess the
validity of such a claim?

To drive home that this is a rigorous question free of metaphysics, we
can put it in software engineering terms: Suppose a piece of software
is said to use phrase N to refer to some object O.  How do we write a
unit test for that property?  Or, how would we detect a bug in the
program caused by an error in reference?

The theory leads to the following definition of reference:

    #+BEGIN_QUOTE
    A noun phrase N refers to object O iff for every sentence S having
    N as its subject phrase, S means a proposition that is about O.
    #+END_QUOTE

(See above for 'means', 'proposition', and 'about'.)

This would predict, for example, that in learning 'what N refers to', an
agent learns the sayability of a number of sentences S that lexically
include N, and interpolates an object hypothesis (the referent of N)
that goes with the propositions that are the meanings of the sentences
S.

Every part of the theory rests on a foundation of variables, sentences,
and sayability.  These are all external phenomena that can be observed
and measured.  There is no appeal to 'mental models' or 'concepts'.

We are led to this reduction because the theory provides no
other way to define reference.

How well this matches the way "reference" is used in ordinary language
remains to be seen.

** Predication

For compositionality, we need for both subjects and predicates to have
meaning that enables their use in new sentences.  Object hypotheses
liberate noun phrases from the sentences they inhabit and permit them
to join with new predicates, but we must also have some theory of the
independence of predicates.

I've been so busy with reference I haven't had time to nail this
down.  But my feeling is that it will end up being much easier than
reference.  My working hypothesis is that a predicate means a 'procedure'
that acts quasi-computationally on an object hypothesis to yield a truth
value.

(Actually an agent will have multiple 'competencies' around subjects
and predicates, not just for assessing truth but also for bringing it
about, as for the interpretation of imperatives.)

An important case to consider is requests to make things.  "Make me an
omelet" has a reference to an omelet that does not yet exist, but will
exist after the request is carried out.  Computationally, the
predicate "Make me ---" operates not on the omelet, but on the omelet
hypothesis.  The hypothesis in turn can be consulted to determine what
ingredients should be used, by asking it what one would observe should
the request be successfully carried out.

** Assessing meaning and reference

Assays of sentence meaning (sayability) cannot be exhaustive because
we would have to measure payoffs in all possible situations, while
controlling for agents' memories (experience).  This might be possible
in a laboratory setting, but is not practical in any realistic
setting.  We can, however, make pretty good hypotheses of meaning with
limited data, by reasoning about agents and environments (using our own knowledge of
them) and applying common sense assumptions to seek the best hypotheses
of meaning that fit available data.

Similarly, because there are so many predicate phrases that might
combine with a given noun phrase to form sentences, we cannot
enumerate and test them all, and we may have to use heuristics to
determine reference.

These definitions of meaning and reference may be exact, but in
practice, meaning and reference are unknowable.  This may feel
unsatisfactory, but remember that there is no definite knowledge in
science at all, only hypotheses that fit the available data better or
worse than one another.

** Unit tests

[Placeholder.  If I'm right then I've established that a computer
really is capable of genuine meaning and reference, not just "form
filling", but only under certain circumstances.  I should be able to
spell out the implications of the theory for 'knowledge
representation' and robot language.]

* Other topics TBD
** Prior work

Leibniz, Frege, Russell, Wittgenstein, Quine, Millikan, Horwich,
Kripke, Gopnik, Harman, Yablo, many others.

Much indebted to Brian Cantwell Smith.

Not totally happy with Chomsky.

** Correction

Systems are typically not stable without mechanisms for correcting
deviations from 'normal' states.

When the system involves language, this means steering agents to
repair 'incorrect' behavior.

Incentives to make corrections have to come from within the system; in
the present case, from agent behaviors, which are guided by payoff
calculations.

So it would be nice to analyze payoffs and stability in relation to
sentence meaning: sentences that shouldn't be understood but are, or
shouldn't be said but are, and why they might be cooperative anyhow.

** Objects change

In order to make use of an object hypothesis when appropriate an agent
must be able to discriminate situations where the hypothesis is likely
to work (the object is 'identified') and those where it is not (what
is seen is not 'identified' as the object).

The theory implies some position on the Ship of Theseus.  What is it?

** Mereology

An object, and a part of that object, require different object
hypotheses.  Explain.

** Species
** Parsing
** Child development

Infants learn meaning quickly and apparently with very little data.
Is what an infant does consistent with what I've outlined?

** What does this have to do with HTTPrange-14?

The infamous HTTPrange-14 question hinged on what a particular kind of
URL (or URI) refers to, and years of bickering by many very clever
people didn't lead to any progress on the question.

https://en.wikipedia.org/wiki/HTTPRange-14

Standards are most successful when they are accompanied by good unit
tests, so in order to steer the group away from metaphysics and
bullying, I asked the question, how would someone write a unit test to
detect variance against _any_ requirement having to do with reference?
There was no answer to this question.
